import pandas as pd

# Step 1: Load your Excel file into a DataFrame
df = pd.read_csv('./original dataset.csv')

# Step 2: Create a list of new row names (index names)

new_row_names = [
    'frame.interface_id',
    'frame.dlt',
    'frame.offset_shift',
    'frame.time_epoch',
    'frame.time_delta',
    'frame.time_delta_displayed',
    'frame.time_relative',
    'frame.len',
    'frame.cap_len',
    'frame.marked',
    'frame.ignored',
    'radiotap.version',
    'radiotap.pad',
    'radiotap.length',
    'radiotap.present.tsft',
    'radiotap.present.flags',
    'radiotap.present.rate',
    'radiotap.present.channel',
    'radiotap.present.fhss',
    'radiotap.present.dbm_antsignal',
    'radiotap.present.dbm_antnoise',
    'radiotap.present.lock_quality',
    'radiotap.present.tx_attenuation',
    'radiotap.present.db_tx_attenuation',
    'radiotap.present.dbm_tx_power',
    'radiotap.present.antenna',
    'radiotap.present.db_antsignal',
    'radiotap.present.db_antnoise',
    'radiotap.present.rxflags',
    'radiotap.present.xchannel',
    'radiotap.present.mcs',
    'radiotap.present.ampdu',
    'radiotap.present.vht',
    'radiotap.present.reserved',
    'radiotap.present.rtap_ns',
    'radiotap.present.vendor_ns',
    'radiotap.present.ext',
    'radiotap.mactime',
    'radiotap.flags.cfp',
    'radiotap.flags.preamble',
    'radiotap.flags.wep',
    'radiotap.flags.frag',
    'radiotap.flags.fcs',
    'radiotap.flags.datapad',
    'radiotap.flags.badfcs',
    'radiotap.flags.shortgi',
    'radiotap.datarate',
    'radiotap.channel.freq',
    'radiotap.channel.type.turbo',
    'radiotap.channel.type.cck',
    'radiotap.channel.type.ofdm',
    'radiotap.channel.type.2ghz',
    'radiotap.channel.type.5ghz',
    'radiotap.channel.type.passive',
    'radiotap.channel.type.dynamic',
    'radiotap.channel.type.gfsk',
    'radiotap.channel.type.gsm',
    'radiotap.channel.type.sturbo',
    'radiotap.channel.type.half',
    'radiotap.channel.type.quarter',
    'radiotap.dbm_antsignal',
    'radiotap.antenna',
    'radiotap.rxflags.badplcp',
    'wlan.fc.type_subtype',
    'wlan.fc.version',
    'wlan.fc.type',
    'wlan.fc.subtype',
    'wlan.fc.ds',
    'wlan.fc.frag',
    'wlan.fc.retry',
    'wlan.fc.pwrmgt',
    'wlan.fc.moredata',
    'wlan.fc.protected',
    'wlan.fc.order',
    'wlan.duration',
    'wlan.ra',
    'wlan.da',
    'wlan.ta',
    'wlan.sa',
    'wlan.bssid',
    'wlan.frag',
    'wlan.seq',
    'wlan.bar.type',
    'wlan.ba.control.ackpolicy',
    'wlan.ba.control.multitid',
    'wlan.ba.control.cbitmap',
    'wlan.bar.compressed.tidinfo',
    'wlan.ba.bm',
    'wlan.fcs_good',
    'wlan_mgt.fixed.capabilities.ess',
    'wlan_mgt.fixed.capabilities.ibss',
    'wlan_mgt.fixed.capabilities.cfpoll.ap',
    'wlan_mgt.fixed.capabilities.privacy',
    'wlan_mgt.fixed.capabilities.preamble',
    'wlan_mgt.fixed.capabilities.pbcc',
    'wlan_mgt.fixed.capabilities.agility',
    'wlan_mgt.fixed.capabilities.spec_man',
    'wlan_mgt.fixed.capabilities.short_slot_time',
    'wlan_mgt.fixed.capabilities.apsd',
    'wlan_mgt.fixed.capabilities.radio_measurement',
    'wlan_mgt.fixed.capabilities.dsss_ofdm',
    'wlan_mgt.fixed.capabilities.del_blk_ack',
    'wlan_mgt.fixed.capabilities.imm_blk_ack',
    'wlan_mgt.fixed.listen_ival',
    'wlan_mgt.fixed.current_ap',
    'wlan_mgt.fixed.status_code',
    'wlan_mgt.fixed.timestamp',
    'wlan_mgt.fixed.beacon',
    'wlan_mgt.fixed.aid',
    'wlan_mgt.fixed.reason_code',
    'wlan_mgt.fixed.auth.alg',
    'wlan_mgt.fixed.auth_seq',
    'wlan_mgt.fixed.category_code',
    'wlan_mgt.fixed.htact',
    'wlan_mgt.fixed.chanwidth',
    'wlan_mgt.fixed.fragment',
    'wlan_mgt.fixed.sequence',
    'wlan_mgt.tagged.all',
    'wlan_mgt.ssid',
    'wlan_mgt.ds.current_channel',
    'wlan_mgt.tim.dtim_count',
    'wlan_mgt.tim.dtim_period',
    'wlan_mgt.tim.bmapctl.multicast',
    'wlan_mgt.tim.bmapctl.offset',
    'wlan_mgt.country_info.environment',
    'wlan_mgt.rsn.version',
    'wlan_mgt.rsn.gcs.type',
    'wlan_mgt.rsn.pcs.count',
    'wlan_mgt.rsn.akms.count',
    'wlan_mgt.rsn.akms.type',
    'wlan_mgt.rsn.capabilities.preauth',
    'wlan_mgt.rsn.capabilities.no_pairwise',
    'wlan_mgt.rsn.capabilities.ptksa_replay_counter',
    'wlan_mgt.rsn.capabilities.gtksa_replay_counter',
    'wlan_mgt.rsn.capabilities.mfpr',
    'wlan_mgt.rsn.capabilities.mfpc',
    'wlan_mgt.rsn.capabilities.peerkey',
    'wlan_mgt.tcprep.trsmt_pow',
    'wlan_mgt.tcprep.link_mrg',
    'wlan.wep.iv',
    'wlan.wep.key',
    'wlan.wep.icv',
    'wlan.tkip.extiv',
    'wlan.ccmp.extiv',
    'wlan.qos.tid',
    'wlan.qos.priority',
    'wlan.qos.eosp',
    'wlan.qos.ack',
    'wlan.qos.amsdupresent',
    'wlan.qos.buf_state_indicated',
    'wlan.qos.bit4',
    'wlan.qos.txop_dur_req',
    'wlan.qos.buf_state_indicated',
    'data.len',
    'class'
     # Ensure 'class' is included
]


  # Replace with your actual list of names

# Step 3: Replace the row names (index) with the new names
df.columns = new_row_names

# Step 4: Save the updated DataFrame back to an Excel file
df.to_csv('updated_excel_file.csv', index=True)
import numpy as np

df.replace('?', np.nan, inplace=True)

# Convert all columns to numeric values, coercing when necessary
df = df.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with the median of each column
df.fillna(df.median(), inplace=True)
print("Are there any NaN values left?", df.isna().sum().sum())


if 'class' in df.columns:
    df['class'] = df['class'].astype(str)
    df['label'] = df['class'].apply(lambda x: 0 if x.strip().lower() == 'normal' else 1)
    df = df.drop(columns=['class'])
else:
    raise ValueError("The 'class' column is missing from the dataset.")

output_file_path = './updated_excel_file.csv.csv'
df.to_csv(output_file_path, index=False)
print(f"Labeling completed and saved to {output_file_path}.")


from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif


df = pd.read_csv(output_file_path)

# Separate features and target
X = df.drop(columns=['label'])
y = df['label']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Are there any NaN values left after scaling?", np.isnan(X_scaled).sum())

if np.isnan(X_scaled).sum() > 0:
    X_scaled = np.nan_to_num(X_scaled, nan=np.nanmean(X_scaled))

selector = SelectKBest(score_func=f_classif, k=20)  # Adjust k based on your needs
X_new = selector.fit_transform(X_scaled, y)

# Get the selected feature names
selected_features = X.columns[selector.get_support()]
print("Selected features:", selected_features)

print("Are there any NaN values left after scaling?", np.isnan(X_scaled).sum())

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import numpy as np

class IDS_Environment:
    def __init__(self, X, y):
        self.X = X
        self.y = y
        self.n_actions = 2  # Normal or Intrusion (0 or 1)
        self.current_index = 0
        
    def reset(self):
        self.current_index = 0
        return self.X[self.current_index]

    def step(self, action):
        reward = 1 if action == self.y[self.current_index] else -1
        self.current_index += 1
        
        done = self.current_index >= len(self.y)
        next_state = self.X[self.current_index] if not done else None
        
        return next_state, reward, done

# Initialize the environment
env = IDS_Environment(X_new, y)


class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, output_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Initialize DQN
input_dim = X_new.shape[1]
output_dim = env.n_actions
dqn = DQN(input_dim, output_dim)

# Loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.SGD(dqn.parameters(), lr=0.01, momentum=0.9)

import torch
import torch.nn as nn
import torch.optim as optim
import random

class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, output_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Initialize DQN
input_dim = X_new.shape[1]  # Assuming X_new is defined
output_dim = env.n_actions  # Assuming env is defined
dqn = DQN(input_dim, output_dim)

# Loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(dqn.parameters(), lr=0.001)  # Reduced learning rate

def train_dqn(env, dqn, optimizer, criterion, n_episodes=5, gamma=0.99, epsilon=0.1):
    dqn.train()
    for episode in range(n_episodes):
        state = env.reset()
        if state is None:
            raise ValueError("The environment reset method returned None.")
        state = torch.FloatTensor(state).unsqueeze(0)  # Shape [1, num_features]

        done = False
        total_loss = 0

        while not done:
            if random.random() < epsilon:
                action = random.choice(range(env.n_actions))  # Ensure action is within valid range
            else:
                with torch.no_grad():
                    q_values = dqn(state)
                    action = q_values.argmax().item()

            next_state, reward, done = env.step(action)
            if next_state is None:
                print(f"Warning: Environment step method returned None for next_state. Resetting environment.")
                next_state = env.reset()  # Reset environment if needed
            next_state = torch.FloatTensor(next_state).unsqueeze(0) if next_state is not None else None
            
            reward = torch.tensor([reward], dtype=torch.float32)
            if next_state is not None:
                with torch.no_grad():
                    max_next_q_value = dqn(next_state).max()
                target = reward + gamma * max_next_q_value
            else:
                target = reward

            # Ensure target and output have consistent shapes
            target = target.unsqueeze(0)  # Shape [1]
            output = dqn(state)[0, action]  # Shape [1]

            # Check for NaN values in target and output
            if torch.isnan(target).any() or torch.isinf(target).any():
                print(f"NaN detected in target")
                continue
            if torch.isnan(output).any() or torch.isinf(output).any():
                print(f"NaN detected in output")
                continue

            loss = criterion(output, target)
            
            # Check for NaN values in loss
            if torch.isnan(loss).any() or torch.isinf(loss).any():
                print(f"NaN detected in loss")
                continue

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(dqn.parameters(), 1.0)  # Gradient clipping
            optimizer.step()

            total_loss += loss.item()
            state = next_state

        print(f"Episode {episode + 1}/{n_episodes}, Loss: {total_loss}")

# Ensure the env's state and action space sizes are consistent with your DQN model.
# Then train the DQN using the function:
train_dqn(env, dqn, optimizer, criterion)

import torch
import numpy as np

def evaluate_dqn(env, dqn, n_episodes=100, epsilon=0.0):
    dqn.eval()  # Set the model to evaluation mode
    total_rewards = []
    for episode in range(n_episodes):
        state = env.reset()
        state = torch.FloatTensor(state).unsqueeze(0)  # Shape [1, num_features]
        done = False
        total_reward = 0

        while not done:
            with torch.no_grad():
                q_values = dqn(state)
                action = q_values.argmax().item()  # Select the action with the highest Q-value

            next_state, reward, done = env.step(action)
            next_state = torch.FloatTensor(next_state).unsqueeze(0) if next_state is not None else None

            total_reward += reward
            state = next_state

        total_rewards.append(total_reward)
        print(f"Episode {episode + 1}/{n_episodes}, Total Reward: {total_reward}")

    average_reward = np.mean(total_rewards)
    print(f"Average Reward over {n_episodes} episodes: {average_reward}")

# Example usage
evaluate_dqn(env, dqn, n_episodes=100)

